{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from chainer.functions import caffe\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 準備用関数\n",
    "def load_image(path, size=None):\n",
    "    \"\"\"sizeがNoneのときは画像のそのままのサイズで読み込む\"\"\"\n",
    "    img = Image.open(os.path.expanduser(path)).convert(\"RGB\")\n",
    "    if size is not None:\n",
    "        img = img.resize(size, Image.BILINEAR)\n",
    "    return tf.constant(transform_for_train(np.array([np.array(img)[:, :, :3]], dtype=np.float32)))\n",
    "\n",
    "\n",
    "def transform_for_train(img):\n",
    "    \"\"\"\n",
    "    読み込む画像がRGBなのに対し、VGGなどのパラメータがBGRの順なので、順番を入れ替える。\n",
    "    ImageNetの色の平均値を引く。\n",
    "    \"\"\"\n",
    "    return img[..., ::-1] - 120\n",
    "\n",
    "\n",
    "def transform_from_train(img):\n",
    "    \"\"\"\n",
    "    transform_for_trainの逆操作。\n",
    "    \"\"\"\n",
    "    data = img[:, :, ::-1] + 120\n",
    "    return data.clip(0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, chainer_conv):\n",
    "        W = chainer_conv.W.data\n",
    "        b = chainer_conv.b.data\n",
    "        self.W = tf.constant(np.transpose(W, [2, 3, 1, 0]))\n",
    "        self.b = tf.constant(b)\n",
    "    \n",
    "    def __call__(self, x, stride=1, activation_fn=tf.nn.relu, padding=\"SAME\"):\n",
    "        y = tf.nn.conv2d(x, self.W, strides=[1, stride, stride, 1], padding=padding) + self.b\n",
    "        return activation_fn(y) if activation_fn else y\n",
    "\n",
    "\n",
    "def pool(x, ksize, stride, padding=\"SAME\"):\n",
    "    return tf.nn.max_pool(x, ksize=[1, ksize, ksize, 1],\n",
    "                           strides=[1, stride, stride, 1],\n",
    "                           padding=padding)\n",
    "\n",
    "\n",
    "def load_caffemodel(caffemodel):\n",
    "    print(\"load model... %s\" % caffemodel)\n",
    "    model = caffe.CaffeFunction(caffemodel)\n",
    "    return lambda layer_name: Conv(getattr(model, layer_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    \"\"\"\n",
    "    特徴量を得るためのモデルのAbstract class\n",
    "    \"\"\"\n",
    "    default_caffemodel = None\n",
    "    default_alpha = None\n",
    "    default_beta = None\n",
    "\n",
    "    def __init__(self, caffemodel=None, alpha=None, beta=None):\n",
    "        self.conv = load_caffemodel(caffemodel or self.default_caffemodel)\n",
    "        self.alpha = alpha or self.default_alpha\n",
    "        self.beta = beta or self.default_beta\n",
    "    \n",
    "        \n",
    "class NIN(BaseModel):\n",
    "    \"\"\"\n",
    "    NINを用いた特徴量\n",
    "    \"\"\"\n",
    "    default_caffemodel = \"nin_imagenet.caffemodel\"\n",
    "    default_alpha = [0., 0., 1., 1.]\n",
    "    default_beta = [1., 1., 1., 1.]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"NINの特徴量\"\"\"\n",
    "        x0 = self.conv(\"conv1\")(x, stride=4, padding=\"VALID\")\n",
    "        \n",
    "        y1 = self.conv(\"cccp2\")(self.conv(\"cccp1\")(x0), activation_fn=None)\n",
    "        pool1 = pool(tf.nn.relu(y1), ksize=3, stride=2)\n",
    "        x1 = self.conv(\"conv2\")(pool1, stride=1)\n",
    "        \n",
    "        y2 = self.conv(\"cccp4\")(self.conv(\"cccp3\")(x1), activation_fn=None)\n",
    "        pool2 = pool(tf.nn.relu(y2), ksize=3, stride=2)\n",
    "        x2 = self.conv(\"conv3\")(pool2, stride=1)\n",
    "\n",
    "        y3 = self.conv(\"cccp6\")(self.conv(\"cccp5\")(x2), activation_fn=None)\n",
    "        pool3 = pool(tf.nn.relu(y3), ksize=3, stride=2)\n",
    "        \n",
    "        drop = tf.nn.dropout(pool3, 0.5)\n",
    "        x3 = self.conv(\"conv4-1024\")(drop)\n",
    "        \n",
    "        return [x0, x1, x2, x3]\n",
    "\n",
    "\n",
    "class VGG(BaseModel):\n",
    "    \"\"\"\n",
    "    VGGを用いた特徴量\n",
    "    \"\"\"\n",
    "    default_caffemodel = \"VGG_ILSVRC_16_layers.caffemodel\"\n",
    "    default_alpha = [0., 0., 1., 1.]\n",
    "    default_beta = [1., 1., 1., 1.]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"VGGの特徴量\"\"\"\n",
    "        y1 = self.conv(\"conv1_2\")(self.conv(\"conv1_1\")(x), activation_fn=None)\n",
    "        x1 = pool(tf.nn.relu(y1), ksize=2, stride=2)\n",
    "\n",
    "        y2 = self.conv(\"conv2_2\")(self.conv(\"conv2_1\")(x1), activation_fn=None)\n",
    "        x2 = pool(tf.nn.relu(y2), ksize=2, stride=2)\n",
    "\n",
    "        y3 = self.conv(\"conv3_3\")(self.conv(\"conv3_2\")(self.conv(\"conv3_1\")(x2)), activation_fn=None)\n",
    "        x3 = pool(tf.nn.relu(y3), ksize=2, stride=2)\n",
    "\n",
    "        y4 = self.conv(\"conv4_3\")(self.conv(\"conv4_2\")(self.conv(\"conv4_1\")(x3)), activation_fn=None)\n",
    "\n",
    "        return [y1, y2, y3, y4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def style_matrix(y):\n",
    "  \"\"\"画風を表現する行列\"\"\"\n",
    "  _, h, w, c = y.get_shape().as_list()\n",
    "  y_reshaped = tf.reshape(y, [-1, h * w, c])\n",
    "  if tf.__version__[0] == '1':\n",
    "    return tf.matmul(y_reshaped, y_reshaped, adjoint_a=True) / (h * w * c)\n",
    "  elif tf.__version__[0] == '0':\n",
    "    return tf.batch_matmul(y_reshaped, y_reshaped, adj_x=True) / (h * w * c)\n",
    "  else:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "y: shape [batch_size, h, w, c]のテンソル\n",
    "（実はbatch_size = 1）\n",
    "\n",
    "style_matrix(y): shape [batch_size, c, c]のテンソル\n",
    "（実質、$c\\times c$の正方行列）\n",
    "\n",
    "$y_{ijk}$ = y[0, i, j, k]とおと、\n",
    "\n",
    "$$\n",
    "\\mathrm{style}(y)_{0ij} = \\frac{1}{hwc}\\sum_{p,q} y_{pqi}y_{pqj}\n",
    "$$\n",
    "を意味する。\n",
    "$hwc$で割っているのはこの値が大きくなりすぎないようにしている。\n",
    "\n",
    "この式は共分散行列の式と比較すると理解しやすいと思います。\n",
    "\n",
    "$m\\times n$行列$\\{X_{ij}\\}$を\n",
    "$(X_{11}, X_{12}, \\dots, X_{1n})$, …, $(X_{m1}, X_{m2}, \\dots, X_{mn})$というように、\n",
    "$m$個の$n$次元ベクトルと解釈します。その共分散行列は、\n",
    "$$\n",
    "\\frac{1}{m}\\sum_k X_{ki}X_{kj} - \\frac{1}{m^2}\\sum_k X_{ki}\\sum_k X_{kj}.\n",
    "$$\n",
    "この式の一つ目の項がstyle_matrixの式で使っているものに相当します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, base_model, img_orig, img_style, config):\n",
    "        # 特徴抽出を行う\n",
    "        mids_orig = base_model(img_orig)\n",
    "        mids_style = base_model(img_style)\n",
    "\n",
    "        # 損失関数に使うものを作る\n",
    "        prods_style = [style_matrix(y) for y in mids_style]\n",
    "\n",
    "        # img_genを初期化する\n",
    "        img_gen = tf.Variable(tf.random_uniform(config.output_shape, -20, 20))\n",
    "\n",
    "        self.img_gen = img_gen\n",
    "        mids = base_model(img_gen)\n",
    "\n",
    "        self.loss_orig = []\n",
    "        self.loss_style = []\n",
    "\n",
    "        for mid, mid_orig in zip(mids, mids_orig):\n",
    "            shape = mid.get_shape().as_list()\n",
    "            self.loss_orig.append(tf.nn.l2_loss(mid - mid_orig) / np.prod(shape))\n",
    "\n",
    "        for mid, prod_style in zip(mids, prods_style):\n",
    "            shape = prod_style.get_shape().as_list()\n",
    "            self.loss_style.append(tf.nn.l2_loss(style_matrix(mid) - prod_style) / np.prod(shape))\n",
    "\n",
    "        total_loss = 0\n",
    "        for l, a in zip(self.loss_orig, base_model.alpha):\n",
    "            if a != 0:\n",
    "                total_loss += l * (a * config.lam)\n",
    "\n",
    "        for l, b in zip(self.loss_style, base_model.beta):\n",
    "            if b != 0:\n",
    "                total_loss += l * b\n",
    "\n",
    "        self.total_loss = total_loss\n",
    "        self.total_train = config.optimizer.minimize(self.total_loss)\n",
    "        clipped = tf.clip_by_value(self.img_gen, -120., 135.)  # 0〜255の範囲に収まるようにする操作\n",
    "        self.clip = tf.assign(self.img_gen, clipped)\n",
    "        \n",
    "    def generate(self, config):\n",
    "        with tf.Session() as sess:\n",
    "            if hasattr(tf, \"global_variables_initializer\"):\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "            else:\n",
    "                sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            print(\"start\")\n",
    "            # 学習開始\n",
    "            for i in range(config.iteration):\n",
    "                sess.run([self.total_train, self.clip])\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    # l, l1, l2 = sess.run([self.total_loss, self.loss_orig, self.loss_style])\n",
    "                    # print(\"%d| loss: %f, loss_orig: %f, loss_style: %f\" % (i + 1, l, sum(l1), sum(l2)))\n",
    "                    # for l1_, l2_ in zip(l1, l2):\n",
    "                    #     print(\"loss_orig: %f, loss_style: %f\" % (l1_, l2_))\n",
    "\n",
    "                    self.save_image(sess, config.save_path % (i + 1))\n",
    "                    \n",
    "    def save_image(self, sess, path):\n",
    "        data = sess.run(self.img_gen)[0]\n",
    "        data = transform_from_train(data)\n",
    "        img = Image.fromarray(data.astype(np.uint8))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(\"save %s\" % path)\n",
    "        img.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_model(model_name, **args):\n",
    "    if model_name == 'nin':\n",
    "        return NIN(**args)\n",
    "    if model_name == 'vgg':\n",
    "        return VGG(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 設定\n",
    "class Config:\n",
    "    batch_size = 1\n",
    "    iteration = 5000\n",
    "    lr = 1.0  # 学習レート\n",
    "    lam = 0.05  # 元画像と画風画像のバランスです。大きくすると元画像寄りになります。\n",
    "    width = 300  # 生成画像の横幅です。小さくすると雑になる代わりに速くなります。\n",
    "    height = 300  # 生成画像の縦幅です。小さくすると雑になる代わりに速くなります。\n",
    "    output_shape = [1, height, width, 3]\n",
    "    output_dir = \"_output\"\n",
    "    model = \"nin\"\n",
    "    # model = \"vgg\"\n",
    "    original_image = \"./images/cat.png\"  # ここに元画像のパスを指定してください\n",
    "    style_image = \"./images/gogh.png\"  # ここに画風画像のパスを指定してください\n",
    "    save_path = os.path.expanduser(os.path.join(output_dir, \"%05d.png\"))\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    no_resize_style = False  # Trueにすると画風画像をリサイズせずに利用する（開始が遅くなる）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "img_orig = load_image(config.original_image, [config.width, config.height])\n",
    "img_style = load_image(config.style_image, [config.width, config.height] if not config.no_resize_style else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = generate_model(config.model)\n",
    "# 読み込みに時間がかかります。\n",
    "\n",
    "# nin = generate_model('nin')\n",
    "# vgg = generate_model('vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 画像生成\n",
    "generator = Generator(model, img_orig, img_style, config)\n",
    "\n",
    "generator.generate(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
